# ğŸ›¡ï¸ RAG Poisoning Lab â€” Educational AI Security Exercise

This repository contains a hands-on lab environment for learning **RAG (Retrieval-Augmented Generation) data poisoning attacks**, detection techniques, and mitigation strategies. It is designed for students, researchers, and security practitioners who want practical experience with adversarial manipulation of AI retrieval systems.

> **âš ï¸ Educational Purpose Only**  
> This lab is intended *strictly* for learning, research, and training.  
> Do **not** use these techniques on any system you do not own or do not have explicit permission to test.

---

## ğŸ“˜ Overview

The lab demonstrates how a RAG system can be poisoned by injecting malicious documents into a vector database. You will:

- Build a simple RAG pipeline using FAISS and Ollama  
- Launch a Streamlit interface to query clean documents  
- Execute poisoning attacks by ingesting malicious files  
- Detect suspicious chunks using perplexity + similarity scoring  
- Apply mitigations such as keyword filters and state resets  

This lab mirrors real-world RAG risks seen in enterprise AI applications.

---

## ğŸš€ Quick Start (Local Setup)

1. Clone the Repository
```bash
git clone https://github.com/r00tb3/RAG-Poisoning-Lab.git
cd rag-poisoning-lab
```

2. Create and Activate Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

3. Install Dependencies
```bash
pip install -r requirements.txt --prefer-binary --no-cache-dir
```

4. Install and Start Ollama
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3:8b-instruct-q4_K_M
ollama pull nomic-embed-text
ollama serve    # keep this running
```

5. Run the Streamlit App
```bash
streamlit run rag_app/app_streamlit.py --server.port 8000
```

6. Open your browser and visit:
http://localhost:8000

7. Folder Structure
```bash
rag-poisoning-lab/
â”‚â”€â”€ rag_app/
â”‚   â”œâ”€â”€ app_streamlit.py      # Main RAG UI
â”‚   â”œâ”€â”€ knowledge_base.py     # Ingestion + mitigation logic
â”‚   â””â”€â”€ detection.py          # Perplexity + similarity detection
â”‚
â”œâ”€â”€ documents/                # Clean knowledge base (15 files)
â”œâ”€â”€ poisoned_docs/            # Malicious payloads (injection, bias, leakage)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```





## ğŸ§ª What You Will Learn ?

ğŸ”´ Attack
- Poisoning via malicious document ingestion

- How semantic similarity causes poisoned chunks to be retrieved

ğŸ” Detect
- Perplexity scoring for unnatural or adversarial text


- Embedding similarity to identify outliers in vector space


ğŸ›¡ï¸ Mitigate
- Keyword filtering during ingestion


- Rebuilding FAISS index to purge poisoned content



- Validating clean behavior after mitigation






- These techniques are essential for securing real-world AI applications.




## ğŸ“œ Disclaimer
_This project is provided for educational, academic, and training purposes only.
Do not use any part of this repository to attack systems without explicit written permission.
The authors assume no liability for misuse._

## â­ Contributing Suggestions and improvements are welcome.
#### _You can submit issues or pull requests to expand:_


- Additional poisoning techniques

- New detection modules

- Hardening strategies for RAG pipelines
